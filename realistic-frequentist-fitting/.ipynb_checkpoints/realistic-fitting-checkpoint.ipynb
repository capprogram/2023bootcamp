{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed171cc0",
   "metadata": {},
   "source": [
    "# Realistic Line Fitting in the Frequentist Paradigm\n",
    "\n",
    "Up to now, we have considered only the simple case of linear fits to y vs. x where all the error is in the y direction, the errors are known (and equal and Gaussian), there are no systematic errors, and the sample is unbiased. Fitting becomes immensely more complex when we relax these assumptions, as partially discussed in Sections 4.2.6-4.2.7 and Chapter 8 of the Ivezic et al. textbook. When there is scatter in both the x and y axes, there is also a decoupling between the \"best fit\" and the \"best prediction\" of y or x (given x or y). Here we will experiment with fake data to get a feel for these issues.\n",
    "\n",
    "Author: Sheila Kannappan\n",
    "Last Modified: May 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports and naming conventions\n",
    "import numpy as np              # basic numerical analysis\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy.random as npr      # random number generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2c448",
   "metadata": {},
   "source": [
    "## A. \"Best\" Fits\n",
    "\n",
    "We'll start with the situation where we are trying to determine the underlying physical relationship between x and y: the \"best fit\" or maximum likelihood estimation (MLE) fit. Correctly determining the MLE fit in the presence of complex errors and biases requires modifying the Likelihood function used in the fitting to reflect the exact details of the situation (e.g. as in Ivezic 4.2.7 or Hogg, Bovy, & Lang 2010); the Likelihood will no longer be simply proportional to exp(-&chi;<sup>2</sup>/2). \n",
    "\n",
    "However, simple frequentist inverse and bisector fits, explored below, can offer a \"quick and dirty\" impression of how much the fit might plausibly change given different assumptions and thus tell you whether doing things correctly is worth the trouble. Unlike a traditional \"forward\" fit, which minimizes residuals in the y direction, an \"inverse\" fit minimizes residuals in the x direction, and a \"bisector\" fit splits the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e43bdc",
   "metadata": {},
   "source": [
    "#### Let's see how these fits look. Using np.linspace, construct two 100-element “data sets” x and y such that x ranges from 0-10 and y ranges from 10-40. Note that x and y should vary smoothly, with no randomness. Plot y vs. x to see the “true” relation with no measurement errors or biases. What are the slope and intercept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create \"perfect\" error-free data and use it to plot underlying true relation\n",
    "ndata=100\n",
    "xx = np.linspace(0,10,ndata)\n",
    "yy = np.linspace(10,40,ndata)\n",
    "xplotrange=np.array([0,10]) # define for plotting fits later, match to min and max of xx above\n",
    "plt.figure(1,figsize=(5,5))\n",
    "plt.clf()\n",
    "plt.plot(xx,yy,color='black',ls=':',label=\"true\")\n",
    "\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce156b8",
   "metadata": {},
   "source": [
    "<font color='blue'>*Answers: The slope is 3 (&Delta;y/&Delta;x = 30/10) and the y-intercept is 10.*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376d52fa",
   "metadata": {},
   "source": [
    "#### Now add random Gaussian scatter to y representing noise with a sigma of 0.8. Plot y vs. x, both with and without the random noise. Fit the data using forward and inverse fits and overplot the fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create noisy data with Gaussian random errors with sigma ~ 0.8\n",
    "sigma = 3*1.8\n",
    "yyscat = yy + npr.normal(0,sigma,ndata) # recall ndata is the number of data pts, set in the previous cell\n",
    "\n",
    "# plot noisy data with scatter\n",
    "plt.plot(xx,yyscat,'r.',label=\"data\")\n",
    "\n",
    "# add forward fit (the usual, y vs. x)\n",
    "addforwardfit = True\n",
    "\n",
    "# add inverse fit (flipped from usual, x vs. y)\n",
    "addinversefit = True\n",
    "\n",
    "if addforwardfit:\n",
    "    pforward = np.polyfit(xx,yyscat,1)\n",
    "    slopefor = pforward[0]\n",
    "    intfor = pforward[1]\n",
    "    plt.plot(xplotrange,slopefor*xplotrange+intfor,'b',label=\"forward\") # forward is blue\n",
    "\n",
    "if addinversefit:\n",
    "    pinverse = np.polyfit(yyscat,xx,1) # notice the fit is minimizing residuals in xx, with yyscat supplied as if on the x-axis\n",
    "    slopeinv = 1./pinverse[0]\n",
    "    intinv = -1.*pinverse[1]/pinverse[0]\n",
    "    plt.plot(xplotrange,slopeinv*xplotrange+intinv,'g',label=\"inverse\") # inverse is green\n",
    "    \n",
    "# re-plot \"true\" relation\n",
    "#plt.plot(xx,yy,color='black',ls=':',label=\"true\")\n",
    "\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61b23da",
   "metadata": {},
   "source": [
    "#### Rerun the cell above, changing the scatter by 0.5 at a time, until you can tell the difference between the forward and inverse fits. How much scatter do you need? Triple that amount so the difference is really obvious. Can you visually see that the scatter is symmetric in y around the forward fit and symmetric in x around the inverse fit?\n",
    "#### Next, uncomment the line of code that overplots the true relation. Which type of fit is closer to the truth? Re-run the code a few times to convince yourself that no matter what random errors you get, that type of fit is always the \"best\" one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af3ea3",
   "metadata": {},
   "source": [
    "<font color='blue'>*Answers: The fits should have been pretty indistinguishable for scatter 0.8 and 1.3, and perceptibly different for 1.8. At 3&times;1.8, the fits should have been obviously different. Overplotting the true relation, you should have found that the forward fit was closer to the truth every time. However, this is BY DEFINITION. We added the scatter only in y, so the forward fit (which minimizes &chi;<sup>2</sup>, so minimizes residuals in y) was guaranteed to provide the best fit.*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f420a668",
   "metadata": {},
   "source": [
    "#### Now add the random Gaussian scatter **to x**. Since the slope is 3, you'll get the same visual effect with 1/3 as much scatter in x as in y. Plot y vs. x, both with and without the random noise. Fit the data using forward and inverse fits and overplot the fits. Which type of fit is closer to the truth? Predict what you think it will be, then uncomment the line of code that overplots the truth. Re-run the code a few times to convince yourself that no matter what random errors you get, that type of fit is always the \"best\" one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16687eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create noisy data with Gaussian random errors in x\n",
    "sigma = 1.8\n",
    "xxscat = xx + npr.normal(0,sigma,ndata) # recall ndata is the number of data pts, set in the previous cell\n",
    "\n",
    "# plot noisy data with scatter\n",
    "plt.plot(xxscat,yy,'r.',label=\"data\")\n",
    "\n",
    "# add forward fit (the usual, y vs. x)\n",
    "addforwardfit = True\n",
    "\n",
    "# add inverse fit (flipped from usual, x vs. y)\n",
    "addinversefit = True\n",
    "\n",
    "if addforwardfit:\n",
    "    pforward = np.polyfit(xxscat,yy,1) # now xx has the scatter\n",
    "    slopefor = pforward[0]\n",
    "    intfor = pforward[1]\n",
    "    plt.plot(xplotrange,slopefor*xplotrange+intfor,'b',label=\"forward\") # forward is blue\n",
    "\n",
    "if addinversefit:\n",
    "    pinverse = np.polyfit(yy,xxscat,1) # now xx has the scatter\n",
    "    slopeinv = 1./pinverse[0]\n",
    "    intinv = -1.*pinverse[1]/pinverse[0]\n",
    "    plt.plot(xplotrange,slopeinv*xplotrange+intinv,'g',label=\"inverse\") # inverse is green\n",
    "    \n",
    "# re-plot \"true\" relation\n",
    "#plt.plot(xx,yy,color='black',ls=':',label=\"true\")\n",
    "\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a942ea62",
   "metadata": {},
   "source": [
    "<font color='blue'>*Answers: With &sigma;=1.8, the fits should have been obviously different. Overplotting the true relation, you should have found that the inverse fit was closer to the truth every time. Again, this is BY DEFINITION, because we added the scatter only in x, and we fed `np.polyfit` the variables in swapped order so when it minimized &chi;<sup>2</sup>, it minimized residuals in x. The inverse fit was guaranteed to provide the best fit.*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce74f36",
   "metadata": {},
   "source": [
    "#### In a realistic fitting situation, we might have errors in both x and y, and we might not know how much error is in each. We might also have systematic errors, which are not random and go preferentially in one direction. The bisector fit, which literally bisects the forward and inverse fits, is the best simple compromise if you have no idea whether the error is mostly in x or mostly in y. Below we define a useful function to compute the bisector slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed633df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisectorslope(fsl,isl):\n",
    "    # function to compute bisector slope from forward and inverse slopes\n",
    "    # using formula in Isobe et al (1990)\n",
    "    bsl1 = (1./(fsl+isl))\n",
    "    bsl2 = (fsl*isl - 1. + np.sqrt((1.+fsl**2)*(1.+isl**2)))\n",
    "    bsl = bsl1*bsl2\n",
    "    return bsl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ff2ea",
   "metadata": {},
   "source": [
    "#### Rerun the cell below several times and compare which fit is closest to the truth in each situation. Again, try to predict the truth before uncommenting the line of code that overplots the truth. Your eye will naturally prefer the bisector fit regardless of the truth. (Use the default example of equal errors in x and y to see this, noting that because of the slope, equal errors means more scatter in x.) However, \"by-eye fitting\" is dangerous -- the bisector is WRONG when the scatter is mostly in one of x or y. You must train your eye to look at the scatter in the direction you believe it comes from, if you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb449a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create noisy data with Gaussian random errors in x\n",
    "sigmax = 3.\n",
    "xxscat = xx + npr.normal(0,sigmax,ndata) # recall ndata is the number of data pts, set in the previous cell\n",
    "\n",
    "# create noisy data with Gaussian random errors in y\n",
    "sigmay = 3.\n",
    "yyscat = yy + npr.normal(0,sigmay,ndata)\n",
    "\n",
    "# add systematic errors\n",
    "syserrs = np.zeros(ndata)\n",
    "dummy = np.arange(0,ndata)\n",
    "num_bad=7\n",
    "syserrs[npr.choice(dummy,size=num_bad,replace=False)] = 5. # use choice to give sys errors to num_bad points, all adding 5.\n",
    "\n",
    "yyscat = yyscat + syserrs # could add syserrs to xxscat instead, comment out to look at only random errors\n",
    "\n",
    "# plot noisy data with scatter\n",
    "plt.plot(xxscat,yyscat,'r.',label=\"data\")\n",
    "\n",
    "# add forward fit (the usual, y vs. x)\n",
    "addforwardfit = True\n",
    "\n",
    "# add inverse fit (flipped from usual, x vs. y)\n",
    "addinversefit = True\n",
    "\n",
    "# add bisector fit (halfway between forward and inverse)\n",
    "addbisectorfit = True\n",
    "\n",
    "if addforwardfit:\n",
    "    pforward = np.polyfit(xxscat,yyscat,1) # use the correct variables -- now both have scatter\n",
    "    slopefor = pforward[0]\n",
    "    intfor = pforward[1]\n",
    "    plt.plot(xplotrange,slopefor*xplotrange+intfor,'b',label=\"forward\") # forward is blue\n",
    "\n",
    "if addinversefit:\n",
    "    pinverse = np.polyfit(yyscat,xxscat,1) # use the correct variables -- now both have scatter\n",
    "    slopeinv = 1./pinverse[0]\n",
    "    intinv = -1.*pinverse[1]/pinverse[0]\n",
    "    plt.plot(xplotrange,slopeinv*xplotrange+intinv,'g',label=\"inverse\") # inverse is green\n",
    "    \n",
    "if addbisectorfit:\n",
    "    slopebis = bisectorslope(slopefor,slopeinv)\n",
    "    intbis = np.mean(yyscat) - slopebis*np.mean(xxscat) # definition of bisector intercept (Isobe et al. 1990)\n",
    "    plt.plot(xplotrange,slopebis*xplotrange+intbis,'m',label=\"bisector\")\n",
    "\n",
    "# re-plot \"true\" relation\n",
    "#plt.plot(xx,yy,color='black',ls=':',label=\"true\")\n",
    "\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a726452",
   "metadata": {},
   "source": [
    "#### Play with the scatter and/or systematic errors in x and y in the cell above, rerunning it with different values to get more intuition for how such simple \"best fits\" depend on which axis dominates as the true source of scatter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248aa7f8",
   "metadata": {},
   "source": [
    "## B. Predictive Fits\n",
    "\n",
    "All of the above assumed that the goal was to measure the true, underlying relationship between x and y. What if your goal were instead to find the best predictive relation between the two, for example to predict y with greatest accuracy for a given x. How would the optimal choice of fit type change in this case? Hint: the best predictive fit provides the output value (at a given input value) about which we can expect equal error toward larger or smaller values. Consider the data and fits below. Which is the best predictive fit for y, given x? Is this fit the best fit compared to the truth? Assuming you know that the scatter is divided between x and y as shown in the code, in which direction should the scatter be symmetric around the best predictive fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create noisy data with Gaussian random errors in x\n",
    "sigmax = 3.\n",
    "xxscat = xx + npr.normal(0,sigmax,ndata) # recall ndata is the number of data pts, set in the previous cell\n",
    "\n",
    "# create noisy data with Gaussian random errors in y\n",
    "sigmay = 3.\n",
    "yyscat = yy + npr.normal(0,sigmay,ndata)\n",
    "\n",
    "# add systematic errors\n",
    "syserrs = np.zeros(ndata)\n",
    "dummy = np.arange(0,ndata)\n",
    "num_bad=7\n",
    "syserrs[npr.choice(dummy,size=num_bad,replace=False)] = 5. # use choice to give sys errors to num_bad points, all adding 5.\n",
    "\n",
    "yyscat = yyscat + syserrs # could add syserrs to xxscat instead, comment out to look at only random errors\n",
    "\n",
    "# plot noisy data with scatter\n",
    "plt.plot(xxscat,yyscat,'r.',label=\"data\")\n",
    "\n",
    "# add forward fit (the usual, y vs. x)\n",
    "addforwardfit = True\n",
    "\n",
    "# add inverse fit (flipped from usual, x vs. y)\n",
    "addinversefit = True\n",
    "\n",
    "# add bisector fit (halfway between forward and inverse)\n",
    "addbisectorfit = True\n",
    "\n",
    "if addforwardfit:\n",
    "    pforward = np.polyfit(xxscat,yyscat,1) # use the correct variables -- now both have scatter\n",
    "    slopefor = pforward[0]\n",
    "    intfor = pforward[1]\n",
    "    plt.plot(xplotrange,slopefor*xplotrange+intfor,'b',label=\"forward\") # forward is blue\n",
    "\n",
    "if addinversefit:\n",
    "    pinverse = np.polyfit(yyscat,xxscat,1) # use the correct variables -- now both have scatter\n",
    "    slopeinv = 1./pinverse[0]\n",
    "    intinv = -1.*pinverse[1]/pinverse[0]\n",
    "    plt.plot(xplotrange,slopeinv*xplotrange+intinv,'g',label=\"inverse\") # inverse is green\n",
    "    \n",
    "if addbisectorfit:\n",
    "    slopebis = bisectorslope(slopefor,slopeinv)\n",
    "    intbis = np.mean(yyscat) - slopebis*np.mean(xxscat)\n",
    "    plt.plot(xplotrange,slopebis*xplotrange+intbis,'m',label=\"bisector\")\n",
    "\n",
    "# re-plot \"true\" relation\n",
    "#plt.plot(xx,yy,color='black',ls=':',label=\"true\")\n",
    "\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc1a8f",
   "metadata": {},
   "source": [
    "<font color='blue'>*Answers: The optimal choice of predictive fit for y, given x, is the forward fit, as its minimization of y-residuals ensures the prediction lies in the middle of the y scatter, so we expect an equal probability of the prediction being too high or too low. This fit is NOT the best fit relative to the truth, unless all the error truly happens to be in the y-direction. Even though the scatter is divided between both x and y in the code, and the closest error to the truth is not the forward fit, nonetheless the best predictive fit for y(x) is the forward fit because the best predictive fit for y(x) must have symmetric scatter in y.*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b5ae62",
   "metadata": {},
   "source": [
    "#### To improve the symmetry of the scatter, we may wish to trim the data in x so that we fit only within a range of x where y shows symmetric scatter. Conversely, if we want to predict x given y, we will use the inverse fit and trim down to a range in y within which the scatter in x is symmetric. Such trimming is valid by definition when trying to construct the optimal predictive relation.\n",
    "\n",
    "### Moral of the story: \n",
    "The notion of a \"best\" fit should be adapted to the question at hand. Even if you just want a quick line fit using a frequentist MLE fitting code, you must think about whether it should be forward, inverse, or bisector based on (a) whether you want to predict one variable from the other or want to understand their true relationship, and (b) in the true relationship case, what information you have regarding which variable drives the scatter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
